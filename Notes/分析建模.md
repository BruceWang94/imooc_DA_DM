# 4.分析建模

## 机器学习与建模

- 学习：通过接收到的数据，归纳提取相同与不同。
- 机器学习：让计算机以数据为基础，进行归纳与总结。
- 模型：数据解释现象的系统。

机器学习根据是否有标注可以分为：
- 监督学习

    有标注的机器学习过程。

    - 分类 - 标注是离散值
    - 回归 - 标注是连续值
- 非监督学习

    没有标注的机器学习过程。

    - 聚类分析
    - 关联分析
- 半监督学习

    部分有标注，部分没有标注。

## 训练集、测试集、验证集

- 训练集：用来训练和拟合模型
- 验证集：当通过训练集训练出多个模型后，使用验证集数据纠偏或比较预测
- 测试集：模型繁华能力的考量

泛化能力：对位置数据的预测能力。
k-fold交叉验证：将数据集分成K份，没分轮流做一遍测试集，其他做训练集。

## 分类

- KNN
- 朴素贝叶斯
- 决策时
- 支持向量机
- 集成方法
- 罗基斯特映射
- 人工神经网络

### KNN（K-Nearest Neighbors）

#### 距离

- 欧式距离

    $$d_{12} = \sqrt{\sum_{k=1}^{n}{(x_{1k} - x_{2k})^2}}$$
- 曼哈顿距离

    $$d_{12} = \sum_{k=1}^{n}{|x_{1k} - x_{2k}|}$$
- 闵可夫斯基距离

    $$d_{12} = \sqrt[p]{\sum_{k=1}^{n}{|x_{1k} - x_{2k}|^p})}$$

#### KD-Tree

通过树形结构

#### 算法思想

### 朴素贝叶斯

- 概率：$P(A)$
- 条件概率：$P(A|B)$， 联合概率：$P(A,B)$

    例如：有5个球，2个白球，3个红球</br>
    随机抽一个球，红球的概率：

    $$P(抽到红球)=3/5$$
    不放回抽，第一次抽到红球，第二次抽到红球的概率：

    $$P(第二次红|第一次红)=2/4，P(第一次红, 第二次红)=6/10$$

- $P(A,B)=P(A|B)P(B)=P(B|A)P(A)$
- 全概率公式：$P(B)=\sum{P(A_i)P(B|A_i)}$
- 贝叶斯公式：$P(B_i|A)=\frac{P(B_i)P(A|B_i)}{\sum{P(B_i)P(A|B_i)}}$

    例如：有5个球，2个白球，3个红球</br>
    抽两次，第二次抽到红球的概率：

    $$P(二红) = P(一白)P(二红|一白) + P(一红)P(二红|一红) = 2/5*3/4 + 3/5*2/4 = 3/5$$

- 例：
    某SNS网站的10000个样本中，真实账号比例$C0=0.89$，虚假账号比例$C1=0.11$。</br>
    $F1$：日志数量/注册天数 $F2<=0.05$, $0.05<F2<0.2$, $F2>0.2$</br>
    $F2$：好友数量/注册天数 $F2<=0.1$, $0.1<F2<0.8$, $F2>0.8$</br>
    $F3$：是否使用真实头像（真实头像为1，非真实头像为0）

    账户状态：$F1=1$（指落入区间1）, $F2=1$, $F3=0$ </br>
    已知条件：</br>
    $P(F1=1|C0)=0.5$，$P(F1=0|C0)=0.5$ </br>
    $P(F2=1|C0)=0.7$，$P(F2=0|C0)=0.2$ </br>
    $P(F3=1|C0)=0.2$，$P(F3=0|C0)=0.9$ </br>

    $$
    \begin{aligned}
    P(C|F1=1, F2=1, F3=0) & = \frac{P(F1=1, F2=1, F3=0|C)P(C)} {P(F1=1, F2=1, F3=0)} \\\\
                          & = \frac{P(F1=1|C)P(F2=1|C)P(F3=0|C)P(C)} {P(F1=1, F2=1, F3=0)}
    \end{aligned}
    $$

    所以：</br>
    $P(F1=1|C0)P(F2=1|C0)P(F3=0|C0)P(C0) = 0.5*0.7*0.2*0.89=0.0623$ </br>
    $P(F1=1|c1)P(F2=1|c1)P(F3=0|c1)P(c1) = 0.1*0.2*0.2*0.11=0.00198$

- Laplace平滑

#### 生成模型与判别模型

- 生成模型：通过求输入与输出的联合概率分布，再求解类别归类的概率。

    $$P(B|A) = \frac{P(A|B)P(B)}{P(A)}$$

- 判别模型：不通过联合概率分布，直接可以可以获得输出对应最大分类的概率。

### 决策树

<table>
    <tr>
        <td align='center'>No.</td>
        <td align='center'>Outlook</td>
        <td align='center'>Temperature</td>
        <td align='center'>Humidity</td>
        <td align='center'>Wind</td>
        <td align='center'>Play</td>
    </tr>
    <tr>
        <td align='center'>1</td>
        <td align='center'>Sunny</td>
        <td align='center'>Hot</td>
        <td align='center'>High</td>
        <td align='center'>Weak</td>
        <td align='center'>No</td>
    </tr>
    <tr>
        <td align='center'>2</td>
        <td align='center'>Sunny</td>
        <td align='center'>Hot</td>
        <td align='center'>High</td>
        <td align='center'>strong</td>
        <td align='center'>No</td>
    </tr>
    <tr>
        <td align='center'>3</td>
        <td align='center'>Overcast</td>
        <td align='center'>Hot</td>
        <td align='center'>High</td>
        <td align='center'>Weak</td>
        <td align='center'>Yes</td>
    </tr>
    <tr>
        <td align='center'>4</td>
        <td align='center'>Rain</td>
        <td align='center'>Mild</td>
        <td align='center'>High</td>
        <td align='center'>Weak</td>
        <td align='center'>Yes</td>
    </tr>
    <tr>
        <td align='center'>5</td>
        <td align='center'>Rain</td>
        <td align='center'>Cool</td>
        <td align='center'>Normal</td>
        <td align='center'>Weak</td>
        <td align='center'>Yes</td>
    </tr>
    <tr>
        <td align='center'>6</td>
        <td align='center'>Rain</td>
        <td align='center'>Cool</td>
        <td align='center'>Normal</td>
        <td align='center'>Strong</td>
        <td align='center'>No</td>
    </tr>
    <tr>
        <td align='center'>7</td>
        <td align='center'>Overcast</td>
        <td align='center'>Cool</td>
        <td align='center'>Normal</td>
        <td align='center'>Strong</td>
        <td align='center'>Yes</td>
    </tr>
    <tr>
        <td align='center'>8</td>
        <td align='center'>Sunny</td>
        <td align='center'>Mild</td>
        <td align='center'>High</td>
        <td align='center'>Weak</td>
        <td align='center'>No</td>
    </tr>
    <tr>
        <td align='center'>9</td>
        <td align='center'>Sunny</td>
        <td align='center'>Cool</td>
        <td align='center'>Normal</td>
        <td align='center'>Weak</td>
        <td align='center'>Yes</td>
    </tr>
    <tr>
        <td align='center'>10</td>
        <td align='center'>Rain</td>
        <td align='center'>Mild</td>
        <td align='center'>Normal</td>
        <td align='center'>Weak</td>
        <td align='center'>Yes</td>
    </tr>
    <tr>
        <td align='center'>11</td>
        <td align='center'>Sunny</td>
        <td align='center'>Mild</td>
        <td align='center'>Normal</td>
        <td align='center'>Strong</td>
        <td align='center'>Yes</td>
    </tr>
    <tr>
        <td align='center'>12</td>
        <td align='center'>Overcast</td>
        <td align='center'>Mild</td>
        <td align='center'>High</td>
        <td align='center'>Strong</td>
        <td align='center'>Yes</td>
    </tr>
    <tr>
        <td align='center'>13</td>
        <td align='center'>Overcast</td>
        <td align='center'>Hot</td>
        <td align='center'>Normal</td>
        <td align='center'>Weak</td>
        <td align='center'>Yes</td>
    </tr>
    <tr>
        <td align='center'>14</td>
        <td align='center'>Rain</td>
        <td align='center'>Mild</td>
        <td align='center'>High</td>
        <td align='center'>Strong</td>
        <td align='center'>No</td>
    </tr>
</table>

- 信息增益-ID3

    $H(X) = - \sum{p_i\log(p_i)}$

    $I(X,Y) = H(Y) - H(Y|X) = H(X) - H(X|Y)$

    例如上例中：
    
    $Entropy(S) = -(9/14) * \log(9/14) - (5/14)*\log(5/14)$

    按Wind切分：

    $$
    \begin{aligned}
    Gain(Wind) & = Entropy(S) - (8/14) * Entropy(Weak) - (6/14) * Entropy(Strong) \\\\
    & = 0.940 - (8/14) * 0.811 - (6/14) * 1.0 \\\\
    & = 0.048
    \end{aligned}
    $$

    同时，$Gain(Humidity)=0.151$，$Gain(Outlook)=0.247$，$Gain(Temperature)=0.029$。
    得到Outlook的增益是最大的，所以按Outlook切分。

- 信息增益率-C4.5

    $GainRatio(X \to Y) = \frac{I(X,Y)}{H(Y)}$

    $GainRation(Outlook) = 0.247/(-4/14*\log(4/14) - 5/14*\log(5/14) - 5/14*\log(5/14)) = 0.156$

- Gini系数-CART

    $Gini(D) = 1 - \sum{(\frac{C_k}{D})^2}$

    Humidity:

    $Gini(High) = 1 - [(3/7)^2 + (4/7)^2] = 0.490$

    $Gini(Normal) = 1 - [(1/7)^2 + (6/7)^2] = 0.345$

    所以

    $Gini(Humidity) = 7/14 * 0.490 + 7/14 * 0.245 = 0.368$

有以下几个问题需要注意：

- 连续值切分 - 计算每个分隔
- 规则用尽 - 投票
- 过拟合 - 修枝剪叶
    - 前剪枝
    - 后剪枝

### 支持向量机（SVM）

$\omega^T = [\omega_0, \omega_1, \cdots, \omega_n]$，$x^T = [x_0, x_1, \cdots, x_n]$

高维面：$\omega^T x + b =0$

分界面：$\omega^T x + b \ge \varepsilon$，$\omega^T x + b \le -\varepsilon$

两侧同时除以$\varepsilon$，可得：$\omega^T x + b \ge 1$，$\omega^T x + b \le -1$

可简写为：$y_i(\omega^T x_i + b) \ge 1$

$\max{\frac{2}{||\omega^2||}}$ 等价于 $\min{\frac{||\omega^2||}{2}}$
$s.t. y_i(\omega^T x_i + b) \ge 1$

通过拉格朗日乘数法，得：

$L = \frac{1}{2} ||\omega^2|| - \sum_{n=1}^{N}{a_n * [y_n(\omega^T x_n + b) - 1]}$

实际情况中，数据不一定线性可分：
有两种思路：

- $L = \frac{1}{2} ||\omega^2|| - \sum_{n=1}^{N}{a_n * [y_n(\omega^T x_n + b) - 1]}$

    存在一些$s.t. y_j(\omega^T x_j + b) \le 1$，求$\max(L)$时，为无穷大。

    那么我们可以通过求$\min(\max(L))$

- 扩维

    低维 -> 高维：

    $(x_1, x_2)$ -> $(x_1, x_2, x_1x_2, x_1^2, x_2^2)$

    $(x_1, x_2, x_3)$ -> $x_1, x_2, x_3, x_1^2, x_1x_2, x_1x3, x_2^2, x_2x_3, x_3^2, x_1^3, x_1^2x_3, x_1^2x_3, x_1x_2x_3, x_1x_2^2, x_1x_2^2, x_1x_3^2, x_2^3, x_2^2x_3, x_2x_3^2, x_3^3$

    $f(x) = \omega x + b$ 低 -> 高 $f(x) = \sum{\omega_i \phi_i(x)} + b$

    $L = \frac{1}{2} ||\omega^2|| - \sum_{n=1}^{N}{a_n * [y_n(\omega^T x_n + b) - 1]}$，求$\min(\max(L))$

    满足KKT条件后，可转化为:

    $L = \sum_{i=1}^{n}{\alpha_i} - \frac{1}{2} \sum_{i,j=1}^{n}{\alpha_i \alpha_j y_i y_j x_i^T x_j}$，求$\max(\min(L))$

    扩维后：

    $L = \sum_{i=1}^{n}{\alpha_i} - \frac{1}{2} \sum_{i,j=1}^{n}{\alpha_i \alpha_j y_i y_j < \phi_i(x_i), \phi_i(x_j)>}$

    例如：

    $p_1, p_2$ -> $(p_1, p_2, p_1p_2, p_1^2, p_2^2)$

    $q_1, q_2$ -> $(q_1, q_2, q_1q_2, q_1^2, q_2^2)$

    扩维后，再内积：

    $p_1q_1 + p_2q_2 + p_1p_2q_1q_2 + p_1^2q_1^2 + p_2^2q_2^2$

    $(<(p_1, p_2), (q_1, q_2)> + 1)^2$

    $2p_1q_1 + 2p_2q_2 + 2p_1p_2q_1q_2 + p_1^2q_1^2 + p_2^2q_2^2 + 1$

    先映射，在计算，会有维度灾难。所以选择先计算（低维空间），在扩维（核函数）

    $f(x) = \sum{\alpha_i y_i < \Phi(x_i) \Phi(x)> + b}$

    常用的核函数：
    - 线性核函数：$\kappa(x, x_i) = x \cdot x_i$
    - 多项式核函数：$\kappa(x, x_i) = ((x \cdot x_i) + 1)^d$
    - 高斯径向基（RBF）核函数：$\kappa(x, x_i) = \exp(-\frac{||x - x_i||^2}{\delta^2})$

    还有几个问题：

    - 少部分异常

        引入松弛变量：

        $\omega^T x_p + b \ge 1 - \zeta$

        $\omega^T x_p + b \le - 1 + \zeta$

    - 样本不平衡

        看场景：可以通过对不同的数据赋予不同的权值

    - 多分类
        - One-Other
        - One-One

### 集成-随机森林&Adaboost

每种算法的复杂程度不同，使用的范围也不同。例如：熵增益的决策树，适合解决离散值比较多的分类问题；而基于Gini系数的CART决策树，适合解决连续值分类的问题；如果标注在空间内隔离性比较好，KNN比较合适；SVM也可以就解决这类问题，同时有一定的抗过拟合的能力，但是运算时间会比较长。

- 组合多个模型，以获得更好的效果

弱可学习、强可学习
强可学习：多项式学习算法的效果较为明显
弱可学习：多项式学习算法的效果不很明显

集成方法：

- 袋装法（Bagging）

    最典型的应用：随机森林

    需要考虑的内容：

    - 树的个数
    - 数的特征数
    - 树的训练集
- 提升法（Boost）

    例如：Adaboost

    优点：

    - 精度高，且灵活可调
    - 几乎不用担心过拟合
    - 简化特征工程流程


## 回归

### 线性回归

### 逻辑回归

### 人工神经网络

### 回归树与提升树

## 聚类

### Kmeans

### DBSCAN

### 层次聚类

### 图分裂

## 关联

### 关联规则

## 半监督-标签传播算法

