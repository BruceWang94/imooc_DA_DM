## 理论铺垫（Part 2）
- 假设检验
- 相关系数：皮尔逊、斯皮尔曼
- 回归：线性回归
- PCA与SVD

### 假设检验
根据一定的假设条件，从样本推算总体或者推断样本与样本之间关系的一种方法。或者，做出一个假设，根据数据或者已知的分布性质来推断这个假设成立的概率有多大。
具体过程：
- 建立原假设$H_0$（包括等号），$H_0$的反命题为$H_1$，也叫备择假设；
- 选择检验统计量；
- 根据显著水平（一般为0.05），确定拒绝域；
- 计算p值或样本统计量，作出判断。

假设检验的方法有很多，它们差别取决于检验统计量的选取上，例如卡方检验、t-分布检验、F检验等。它们的流程是一样的，只是统计量不同，场景也会不同。例如：t-分布检验，常用于比较两组样本分布是否一致；F检验，常用于方差分析。

__假设检验（例）__：</br>
洗衣粉标准重量500g，标准差2g。
501.8、502.4、499、500.3、504.5、498.2、505.6
1. $H_0$：样本是符合均值500g，标准差2g的正态分布。
2. 假设检验量：$\frac{x-x_0}{\sqrt{\sigma^2/n}} = \frac{x-500}{\sqrt{2^2/7}}$
3. 显著性水平：0.05
4. 2.23，拒绝（p值0.026<0.05）

__卡方检验（例）__：</br>
判断化妆行为与性别有没有关系，经调查得到下表：

<table align="center">
    <tr>
        <td></td>
        <td>男</td>
        <td>女</td>
        <td></td>
    </tr>
    <tr>
        <td>化妆</td>
        <td>15(55)</td>
        <td>95(55)</td>
        <td>110</td>
    </tr>
    <tr>
        <td>不化妆</td>
        <td>85(45)</td>
        <td>5(45)</td>
        <td>90</td>
    </tr>
    <tr>
        <td></td>
        <td>100</td>
        <td>100</td>
        <td>200</td>
    </tr>
</table>

确定原假设$H_0$：化妆与性别无关。即，在所有的人群中，化妆与不化妆的人群中的男女分布是一致的。假设检验量就是卡方分布的假设检验量，如下公式：

$$\chi^2 = \sum_{i=1}^{k}{\frac{(f_i-np_i)^2}{np_i}}$$

其中，$f_i$是实际值，$np_i$理论分布，例如男士化妆的实际值是15，而理论值是55。从而得到，

$$
\begin{aligned}
\chi^2 = & \sum_{i=1}^{k}{\frac{(f_i-np_i)^2}{np_i}} \\\\
       = & \frac{(95-55)^2}{55} + \frac{(15-55)^2}{55} + \frac{(85-45)^2}{45} + \frac{(5-45)^2}{45} \\\\
       = & 129.3
\end{aligned}
$$

|P|0.99|0.95|0.90|0.70|0.50|0.30|0.10|0.05|0.01|
|---|---|---|---|---|---|---|---|---|---|---|
|卡方|0.00016|0.004|0.016|0.148|0.455|1.074|2.706|3.841|6.635|

所以，可以拒绝原假设。即，化妆与性别有关。

#### 方差检验
m组，共n个采样：3组，共15个采样

<table align="center">
    <tr>
        <td rowspan='2'>编号</td>
        <td colspan='3'>电池寿命</td>
    </tr>
    <tr>
        <td>甲</td>
        <td>乙</td>
        <td>丙</td>
    </tr>
    <tr>
        <td>1</td>
        <td>49</td>
        <td>28</td>
        <td>38</td>
    </tr>
    <tr>
        <td>2</td>
        <td>50</td>
        <td>32</td>
        <td>40</td>
    </tr>
    <tr>
        <td>3</td>
        <td>39</td>
        <td>30</td>
        <td>45</td>
    </tr>
    <tr>
        <td>4</td>
        <td>40</td>
        <td>26</td>
        <td>42</td>
    </tr>
    <tr>
        <td>5</td>
        <td>43</td>
        <td>34</td>
        <td>48</td>
    </tr>
</table>

方差检验（单因素）
- 计算

    $$SST = \sum_{i=1}^{m} \sum_{j=1}^{n_i}{(x_{ij}-\bar{x})^2}$$

    $$SSM = \sum_{i=1}^{m} \sum_{j=1}^{n_i}{(\bar{x_i}-\bar{x})^2}$$

    $$SSE = \sum_{i=1}^{m} \sum_{j=1}^{n_i}{(x_{ij}-\bar{x_i})^2}$$

    - SS - 平方和
    - SST - 总变差平方和；
    - SSM - 平均平方和；每个组与整体均值的平方和，又称组间平方和；
    - SSE - 残差平方和；每个组内部的平方和，又称组内平方和。

- 检测统计量F，做假设检验【F满足自由度$(m-1,n-m)$的F分布】

    $$F=\frac{SSM / (m-1)}{SSE / (n-m)}$$

确定原假设，假设均值是一样的。假设统计量为F值，显著性水平为0.05。

计算可得，$\bar{x}_1=44.2$，$\bar{x}_2=30$，$\bar{x}_3=42.6$，$\bar{x}=38.93$。

从而，可得$SSM=604.93$，$SSE=206$。

$$F=\frac{SSM / (3-1)}{SSE / (15-3)}=17.62$$

可得$P_{value} = 0.00027<0.05$，拒绝原假设。

### 相关系数：Pearson、Spearman
是衡量两组数据或两组样本的分布趋势、变化趋势、一致性程度的因子。相关有正相关、负相关和不相关。相关系数越大、越近接近于1，则二者变化趋势越正向同步，一个变大，另一个也变大；相关系数越小、越接近于负1，则二者变化趋势越反向同步，一个变大，另一个变小 ；相关系数趋近于0，则二者没有相关关系。

常用的有：Pearson相关系数、Spearman相关系数
- Perason相关系数

    $$r(X,Y) = \frac{Cov(X,Y)}{\sigma_x \sigma_y} = \frac{E[(X-\mu_x)(Y-\mu_y)]}{\sigma_x\sigma_y}$$

    例如：

    $X=(1,0,1), Y=(2,0,2) \to r=1$

    $X=(1,-3,1), Y=(-2,6,-2) \to r=-1$
- Spearman相关系数

    $$\rho_s = 1 - \frac{6\sum{d_i^2}}{n(n^2-1)}$$

    其中，$n$是每组数据的数量，$d_i$是两组数据排名后的名次差。

    例如：

    $X=(6,11,8), \text{Rank\_X}=(1,3,2)$

    $Y=(7,4,3), \text{Rank\_Y}=(3,2,1)$

    可得$d=(-2,1,1)$，通过公式的$\rho_s = -0.5$。


$$$$

### 回归：线性回归
- 回归：确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。

    可通过最小二乘法得到相关的系数：$\hat{y} = \hat{b}x + \hat{a}$，其中

    $$
    \begin{cases}
    \hat{b} = \frac{\sum_{i=1}^{n}{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i=1}^{n}{(x_i-\bar{x})^2}} = \frac{\sum_{i=1}^{n}{x_iy_i-n\bar{x}\bar{y}}}{\sum_{i=1}^{n}{x_i^2 - n\bar{x}}} \\\\
    \hat{a} = \bar{y} - \hat{b} \bar{x}
    \end{cases}
    $$

线性回归的效果的判定主要有以下两种度量：
- 决定系数
    - 一元线性回归的决定系数

        $$R^2 = \frac{SSR}{SST} = \frac{\sum_{i=1}^{n}{(\hat{y}_i - \bar{y})^2}}{(y_i - \bar{y})^2}$$

        其中，$\hat{y}_i$为预测值（估计值），$y_i$为实际值，$\bar{y}$为均值。$R^2$越接近于1，回归效果越好；越接近于0，效果越差。

    - 多元线性回归的决定系数

        $$R^2 = 1 - \frac{SSR/(n-k-1)}{SST/(n-1)}$$

        其中，$k$为参数的个数
- 残差不相关（DW检验）

    $$DW = \frac{\sum_{t=2}^{n}{(e_t-e_{t-1})^2}}{\sum_{t=2}^{n}{e_t^2}}$$

    其中， $e_i$为残差。$DW \in (0,4)$，如果$DW=2$，则残差不相关；如果$DW$接近于4，则残差正相关；如果$DW$接近于0，则残差负相关。一个好的回归，残差应该不相关，即$DW$接近于2

### PCA与SVD
#### 主成分分析（PCA）
在学矩阵的时候，通常把一个数据表看成一个空间或一个矩阵。矩阵的行对应数据的各个属性，矩阵的列对应属性的不同内容。常把整个属性看成构成这个空间的一个维度，每行代表的实体就是一个向量，表的内容就是一个空间。

- 例如：

    <table align="center">
        <tr>
            <td>A</td>
            <td>B</td>
            <td>C</td>
            <td>D</td>
        </tr>
        <tr>
            <td>10</td>
            <td>7</td>
            <td>9</td>
            <td>2</td>
        </tr>
        <tr>
            <td>10</td>
            <td>19</td>
            <td>8</td>
            <td>5</td>
        </tr>
        <tr>
            <td>10</td>
            <td>15</td>
            <td>10</td>
            <td>7</td>
        </tr>
        <tr>
            <td>10</td>
            <td>6</td>
            <td>7</td>
            <td>10</td>
        </tr>
    </table>

    其中，表里有4个属性或维度。维度有主次之分，例如维度A是一个次要的维度，因为不能通过它来区分对象；而维度B的区分度比较大。更甚者，可以通过正交变换，来获得有用的维度。

求解过程如下：
- 求特征协方差矩阵
- 求协方差的特征值和特征向量
- 将特征值按照从大到小的顺序排列，选择其中最大的k个
- 将样本点投影到选择的特征向量上
---
- 例如：数据如下

    $$x = 2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1$$
    $$y = 2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9$$

    求得协方差矩阵

    $$
    cov = 
    \begin{pmatrix}
    .616555556 & .615444444 \\\\
    .615444444 & .716555556
    \end{pmatrix}
    $$

    再求得特征值与特征向量

    $$
    \begin{aligned}
    eigenvalues & = 
        \begin{pmatrix}
        .0490833989 \\\\
        1.28402771
        \end{pmatrix} \\\\
    eigenvectors & = 
        \begin{pmatrix}
        -.735178656 & -.677863399 \\\\
        .677863399 & -.735178656
        \end{pmatrix}
    \end{aligned}
    $$

    可以发现，第二个特征值最大，从而选择第二个特征向量。即

    $$
    v = 
    \begin{pmatrix}
        -.677863399 \\\\
        -.735178656
    \end{pmatrix}
    $$

    通过$\begin{bmatrix} x' & y' \end{bmatrix} \cdot v$，得到主成分

    $$x = -0.827970186, 1.77758033, 0.992197494, \cdots, 1.22382056$$
    其中$x' = x - \bar{x}$，$y' = y - \bar{y}$。

主成分分析最主要的作用是降维。

#### 奇异值分解（SVD）
- 特征矩阵$A$分解为$m \times m$的酉阵$U$，$m \times n$半正定矩阵（奇异值阵），$n \times n$酉阵转置$V$

    $$A = U \Sigma V^T$$

    酉阵在实数域是单位阵，$U$是当前空间下的一组正交基，$V$是在变换后空间下的一组正交基。通过调整奇异值矩阵的奇异值维度，保留较大的奇异值，舍弃较小的奇异值，同时缩小$U$与$V$的规模。就可以尽可能保留信息的情况下，减少数据的维度。

    $$A_{m \times n} \approx U_{m \times r} \Sigma_{r \times r} V_{r \times n}^T$$

    $$U_{r \times m}^T A_{m \times n} \approx  \Sigma_{r \times r} V_{r \times n}^T$$