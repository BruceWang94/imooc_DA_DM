{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] += os.pathsep + '/usr/local/Cellar/graphviz/2.40.1/bin'\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sl: satisfaction_level --- False:MinMaxScaler; True:StandardScaler\n",
    "# le: last_evaluation --- False:MinMaxScaler; True:StandardScaler\n",
    "# npr: number_project --- False:MinMaxScaler; True:StandardScaler\n",
    "# amh: average_monthly_hours --- False:MinMaxScaler; True:StandardScaler\n",
    "# tsc: time_spend_company --- False:MinMaxScaler; True:StandardScaler\n",
    "# wa: Work_accident --- False:MinMaxScaler; True:StandardScaler\n",
    "# pl5: promotion_last_5years --- False:MinMaxScaler; True:StandardScaler\n",
    "# dp: department --- False:LabelEncoder; True:OneHotEncoder\n",
    "# slr: salary --- False:LabelEncoder; True:OneHotEncoder\n",
    "def hr_preprocessing(sl=False, \n",
    "                     le=False, \n",
    "                     npr=False, \n",
    "                     amh=False, \n",
    "                     tsc=False, \n",
    "                     wa=False, \n",
    "                     pl5=False, \n",
    "                     dp=False, \n",
    "                     slr=False, \n",
    "                     lower_d=False, \n",
    "                     ld_n=1):\n",
    "    df = pd.read_csv('./data/HR.csv')\n",
    "    \n",
    "    ## 1. clean the data\n",
    "    #  remove the outliers\n",
    "    df = df.dropna(subset=['satisfaction_level', 'last_evaluation'])\n",
    "    df = df[df['satisfaction_level'] <= 1][df['salary']!='nme']\n",
    "    \n",
    "    ## 2. get the label\n",
    "    label = df['left']\n",
    "    df = df.drop('left', axis=1)\n",
    "\n",
    "    ## 3. feature selection\n",
    "    #  due to few features, we keep all the features\n",
    "    \n",
    "    ## 4. feature preprocessing\n",
    "    scaler_lst = [sl, le, npr, amh, tsc, wa, pl5]\n",
    "    column_lst = ['satisfaction_level', 'last_evaluation', 'number_project', \\\n",
    "                  'average_monthly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            df[column_lst[i]] = \\\n",
    "                MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            df[column_lst[i]] = \\\n",
    "                StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1, -1)[0]\n",
    "    \n",
    "    scaler_lst = [dp, slr]\n",
    "    column_lst = ['department', 'salary']\n",
    "    for i in range(len(scaler_lst)):\n",
    "        if not scaler_lst[i]:\n",
    "            if column_lst[i] == 'salary':\n",
    "                df[column_lst[i]] = [map_salary(s) for s in df['salary'].values]\n",
    "            else:\n",
    "                df[column_lst[i]] = LabelEncoder().fit_transform(df[column_lst[i]])\n",
    "            df[column_lst[i]] = \\\n",
    "                MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-1,1)).reshape(1, -1)[0]\n",
    "        else:\n",
    "            df = pd.get_dummies(df, columns=[column_lst[i]])\n",
    "    if lower_d:\n",
    "        #return LinearDiscriminantAnalysis(n_components=ld_n)\n",
    "        return PCA(n_components=ld_n).fit_transform(df.values)\n",
    "    return df, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict([('low', 0), ('medium', 1), ('high', 2)])\n",
    "def map_salary(s):\n",
    "    return d.get(s, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr_modeling(features, label):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    f_v = features.values\n",
    "    f_names = features.columns.values\n",
    "    l_v = label.values\n",
    "    X_tt, X_validation, Y_tt, Y_validation = train_test_split(f_v, l_v, test_size=0.2)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_tt, Y_tt, test_size=0.25)\n",
    "    \n",
    "    \n",
    "    ## KNN\n",
    "    from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "    from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "    from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "    from sklearn.externals.six import StringIO\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    \n",
    "    models = []\n",
    "    models.append(('KNN', KNeighborsClassifier(n_neighbors=3)))\n",
    "    models.append(('GaussianNB', GaussianNB()))\n",
    "    models.append(('BernoulliNB', BernoulliNB()))\n",
    "    models.append(('DecesionTreeGini', DecisionTreeClassifier()))\n",
    "    models.append(('DecesionTreeEntropy', DecisionTreeClassifier(criterion='entropy')))\n",
    "    models.append(('SVM Classifier', SVC(C=100)))\n",
    "    models.append(('RandomForest', RandomForestClassifier(max_features=None, bootstrap=False)))\n",
    "    models.append(('Adaboost', AdaBoostClassifier(n_estimators=100)))\n",
    "    \n",
    "    for clf_name, clf in models:\n",
    "        clf.fit(X_train, Y_train)\n",
    "        \n",
    "        xy_lst = [(X_train, Y_train), (X_validation, Y_validation),(X_test, Y_test)]\n",
    "        for i in range(len(xy_lst)):\n",
    "            X_part = xy_lst[i][0]\n",
    "            Y_part = xy_lst[i][1]\n",
    "            \n",
    "            Y_pred = clf.predict(X_part)\n",
    "            \n",
    "            print (i)\n",
    "            print (clf_name, '-ACC:',accuracy_score(Y_part, Y_pred))\n",
    "            print (clf_name, '-REC:',recall_score(Y_part, Y_pred))\n",
    "            print (clf_name, '-F1:',f1_score(Y_part, Y_pred))\n",
    "            \n",
    "            ### draw the Decision Tree\n",
    "            ## version 1\n",
    "#             dot_data = export_graphviz(clf, \n",
    "#                                        out_file=None, \n",
    "#                                        feature_names=f_names,\n",
    "#                                        class_names=['NL', 'L'], \n",
    "#                                        filled=True, \n",
    "#                                        rounded=True, \n",
    "#                                        special_characters=True)\n",
    "#             graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "#             graph.write_pdf('dt_tree.pdf')\n",
    "            \n",
    "            ## version 2\n",
    "#             dot_data = StringIO()\n",
    "#             export_graphviz(clf,\n",
    "#                             out_file=dot_data, \n",
    "#                             feature_names=f_names,\n",
    "#                             class_names=['NL', 'L'], \n",
    "#                             filled=True, \n",
    "#                             rounded=True, \n",
    "#                             special_characters=True)\n",
    "#             graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "#             graph.write_pdf('dt_tree_2.pdf')\n",
    "  \n",
    "    '''\n",
    "    ## save model\n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(knn_clf, 'knn_clf')\n",
    "    ## use model\n",
    "    knn_clfjoblib.load('knn_clf')\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regr_test(features, label):\n",
    "    #print (\"X\", features)\n",
    "    #print (\"Y\", label)\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "    #regr = LinearRegression()\n",
    "    #regr = Ridge(alpha=0.8)\n",
    "    regr = Lasso(alpha=0.002)\n",
    "    regr.fit(features.values, label.values)\n",
    "    \n",
    "    Y_pred = regr.predict(features.values)\n",
    "    print ('Coef:', regr.coef_)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "    print (\"MSE:\", mean_squared_error(Y_pred, label.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    features, label = hr_preprocessing()\n",
    "    regr_test(features[['number_project', 'average_monthly_hours']], features['last_evaluation'])\n",
    "    #hr_modeling(features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef: [0.25039551 0.24227119]\n",
      "MSE: 0.0596363767370062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiyuwang/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/shiyuwang/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/shiyuwang/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/shiyuwang/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/shiyuwang/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/shiyuwang/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/shiyuwang/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
